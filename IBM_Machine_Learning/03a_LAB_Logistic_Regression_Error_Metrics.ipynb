{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac18046-cebf-436d-bedd-d081d74a1139",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 3, Part a: Logistic Regression LAB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632e430-1941-4f46-a10a-8858b578dc5d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using the [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) database, which was built from the recordings of study participants who carried a smartphone with an embedded inertial sensor while performing activities of daily living (ADL). The objective is to classify the activities the participants performed into one of the six following categories: walking, walking upstairs, walking downstairs, sitting, standing, and laying.\n",
    "\n",
    "The following information is provided for each record in the dataset: \n",
    "\n",
    "- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration \n",
    "- Triaxial Angular velocity from the gyroscope \n",
    "- A 561-feature vector with time and frequency domain variables \n",
    "- The activity label \n",
    "\n",
    "More information about the features are available on the website linked above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266ce09-d2e6-418c-ba28-23a5963f63c3",
   "metadata": {},
   "source": [
    "Install the below libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3897be-3468-4b44-8b6b-0a2444efb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn  \n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751465de-8ce8-40ae-b8ae-b2cb1c32a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe4223b-d710-41e0-8bf9-7e242386809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns, pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f123600-6e87-4440-9f90-cf34d0f0dd6c",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Import the data and do the following:\n",
    "\n",
    "* Examine the data types--there are many columns, so it might be wise to use value counts.\n",
    "* Determine if the floating point values need to be scaled.\n",
    "* Determine the breakdown of each activity.\n",
    "* Encode the activity label as an integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50433dd-e6cf-4ee3-a44c-2a5e09376138",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/Human_Activity_Recognition_Using_Smartphones_Data.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f32107-9a24-46d5-97a4-5ae9698e4301",
   "metadata": {},
   "source": [
    "The data columns are all floats except for the activity label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7bc97df-7d56-4b14-ad21-30a758f42979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    561\n",
       "object       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6207abe2-587a-4ead-879b-46db4a9b7066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angle(tBodyGyroJerkMean,gravityMean)    float64\n",
       "angle(X,gravityMean)                    float64\n",
       "angle(Y,gravityMean)                    float64\n",
       "angle(Z,gravityMean)                    float64\n",
       "Activity                                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8bf93-ee2c-4d66-a406-fda1696ee3f7",
   "metadata": {},
   "source": [
    "The data are all scaled from -1 (minimum) to 1.0 (maximum).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a40e426-6a3f-457d-ab84-d3873db518e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    561\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, :-1].min().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6105a668-9afe-42b9-9d36-0c3189731c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    561\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, :-1].max().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287e7f8-27a8-41b5-9cc9-498a2bfe91a8",
   "metadata": {},
   "source": [
    "Examine the breakdown of activities; they are relatively balanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b212b79c-4f44-4979-8019-28eb64b2d62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Activity\n",
       "LAYING                1944\n",
       "STANDING              1906\n",
       "SITTING               1777\n",
       "WALKING               1722\n",
       "WALKING_UPSTAIRS      1544\n",
       "WALKING_DOWNSTAIRS    1406\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195b9fa-53b4-46e2-a189-e5ec43689ca2",
   "metadata": {},
   "source": [
    "Scikit learn classifiers won't accept a sparse matrix for the prediction column. Thus, either `LabelEncoder` needs to be used to convert the activity labels to integers, or if `DictVectorizer` is used, the resulting matrix must be converted to a non-sparse array.  \n",
    "Use `LabelEncoder` to fit_transform the \"Activity\" column, and look at 5 random values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f11d866-9827-434f-b472-24e769794054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3683    0\n",
       "8095    5\n",
       "5187    3\n",
       "622     3\n",
       "5466    5\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Activity'] = le.fit_transform(data.Activity)\n",
    "data['Activity'].sample(5)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061ff28-cc8c-4909-abcc-faa00de32f85",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Calculate the correlations between the dependent variables.\n",
    "* Create a histogram of the correlation values.\n",
    "* Identify those that are most correlated (either positively or negatively).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d2ca058-c3fb-4c7a-a950-5304b7dcf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Calculate the correlation values\n",
    "feature_cols = data.columns[:-1]\n",
    "corr_values = data[feature_cols].corr()\n",
    "\n",
    "# Simplify by emptying all the data below the diagonal\n",
    "tril_index = np.tril_indices_from(corr_values)\n",
    "\n",
    "# Make the unused values NaNs\n",
    "for coord in zip(*tril_index):\n",
    "    corr_values.iloc[coord[0], coord[1]] = np.nan\n",
    "    \n",
    "# Stack the data and convert to a data frame\n",
    "corr_values = (corr_values\n",
    "               .stack()\n",
    "               .to_frame()\n",
    "               .reset_index()\n",
    "               .rename(columns={'level_0':'feature1',\n",
    "                                'level_1':'feature2',\n",
    "                                0:'correlation'}))\n",
    "\n",
    "# Get the absolute values for sorting\n",
    "corr_values['abs_correlation'] = corr_values.correlation.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64522b4d-2b71-4d02-8597-0ed2d4863d6f",
   "metadata": {},
   "source": [
    "A histogram of the absolute value correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df13a28-939e-4cf2-a131-e73fa993c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2e5ee-162a-4246-a26e-27ad83047814",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = corr_values.abs_correlation.hist(bins=50, figsize=(12, 8))\n",
    "ax.set(xlabel='Absolute Correlation', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d8984-8c73-4dae-9fcd-16292da81808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most highly correlated values\n",
    "corr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea27d163-ba6c-4bdf-b1f3-523f71cbf26c",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Split the data into train and test data sets. This can be done using any method, but consider using Scikit-learn's `StratifiedShuffleSplit` to maintain the same ratio of predictor classes.\n",
    "* Regardless of the method used to split the data, compare the ratio of classes in both the train and test splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7399bd5-9088-4df0-9c14-791d30b096b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Get the split indexes\n",
    "strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n",
    "                                          test_size=0.3, \n",
    "                                          random_state=42)\n",
    "\n",
    "train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.Activity))\n",
    "\n",
    "# Create the dataframes\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'Activity']\n",
    "\n",
    "X_test  = data.loc[test_idx, feature_cols]\n",
    "y_test  = data.loc[test_idx, 'Activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd01f68d-fda5-4d8f-bc94-5fc8549c4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae85b07-a5ad-46bd-8349-f394d0b46f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c50e7-56e3-446d-93e2-040b83fde09a",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Fit a logistic regression model without any regularization using all of the features. Be sure to read the documentation about fitting a multi-class model so you understand the coefficient output. Store the model.\n",
    "* Using cross validation to determine the hyperparameters and fit models using L1 and L2 regularization. Store each of these models as well. Note the limitations on multi-class models, solvers, and regularizations. The regularized models, in particular the L1 model, will probably take a while to fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0e439-a66e-43b4-afa6-84c1dd050d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Standard logistic regression\n",
    "lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a7ee6-7994-41e2-8752-e65d6b250a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# L1 regularized logistic regression\n",
    "lr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefaf79a-9cba-4c90-9d20-aa97484ecc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularized logistic regression\n",
    "lr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear').fit(X_train, y_train)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890639e-240c-4369-8663-05cd19782c54",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "* Compare the magnitudes of the coefficients for each of the models. If one-vs-rest fitting was used, each set of coefficients can be plotted separately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec43c9a-1861-416a-b6dd-cf9670f32e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Combine all the coefficients into a dataframe\n",
    "coefficients = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    coeffs = mod.coef_\n",
    "    coeff_label = pd.MultiIndex(levels=[[lab], [0,1,2,3,4,5]], \n",
    "                                 codes=[[0,0,0,0,0,0], [0,1,2,3,4,5]])\n",
    "    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n",
    "\n",
    "coefficients = pd.concat(coefficients, axis=1)\n",
    "\n",
    "coefficients.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684e913-dfe1-4c08-a81b-8814f740161d",
   "metadata": {},
   "source": [
    "Prepare six separate plots for each of the multi-class coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b97063-d360-4f7b-ace2-eceacdc8ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axList = plt.subplots(nrows=3, ncols=2)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(10,10)\n",
    "\n",
    "for ax in enumerate(axList):\n",
    "    loc = ax[0]\n",
    "    ax = ax[1]\n",
    "    \n",
    "    data = coefficients.xs(loc, level=1, axis=1)\n",
    "    data.plot(marker='o', ls='', ms=2.0, ax=ax, legend=False)\n",
    "    \n",
    "    if ax is axList[0]:\n",
    "        ax.legend(loc=4)\n",
    "        \n",
    "    ax.set(title='Coefficient Set '+str(loc))\n",
    "\n",
    "plt.tight_layout()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b0a68-e433-4f10-b90d-0d5936f10c65",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "* Predict and store the class for each model.\n",
    "* Store the probability for the predicted class for each model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160cfaf-b696-42ec-8195-89e3f6b14064",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Predict the class and the probability for each\n",
    "y_pred = list()\n",
    "y_prob = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n",
    "    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n",
    "    \n",
    "y_pred = pd.concat(y_pred, axis=1)\n",
    "y_prob = pd.concat(y_prob, axis=1)\n",
    "\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4a498-e4d5-4166-b3bc-0d4f78a8f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob.head()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c6141-f9e4-4460-a39a-4fa2fe967f17",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "For each model, calculate the following error metrics: \n",
    "\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F-score\n",
    "* Confusion Matrix\n",
    "\n",
    "Decide how to combine the multi-class metrics into a single value for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f510ed2-e0b1-4bf4-a7f9-6cf433729c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "for lab in coeff_labels:\n",
    "\n",
    "    # Preciision, recall, f-score from the multi-class support function\n",
    "    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n",
    "    \n",
    "    # The usual way to calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred[lab])\n",
    "    \n",
    "    # ROC-AUC scores can be calculated by binarizing the data\n",
    "    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n",
    "              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n",
    "              average='weighted')\n",
    "    \n",
    "    # Last, the confusion matrix\n",
    "    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n",
    "    \n",
    "    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n",
    "                              'fscore':fscore, 'accuracy':accuracy,\n",
    "                              'auc':auc}, \n",
    "                             name=lab))\n",
    "\n",
    "metrics = pd.concat(metrics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d803c69-afe1-4abb-a31d-47a185e9de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b8c2-2074-4489-bf28-25e3292aaefe",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "* Display or plot the confusion matrix for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8bace-7502-4e59-a76e-fb0b0111bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "fig, axList = plt.subplots(nrows=2, ncols=2)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(12, 10)\n",
    "\n",
    "axList[-1].axis('off')\n",
    "\n",
    "for ax,lab in zip(axList[:-1], coeff_labels):\n",
    "    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d');\n",
    "    ax.set(title=lab);\n",
    "    \n",
    "plt.tight_layout()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b78a1-b44d-4c1a-b2b9-f898180c36a4",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "prev_pub_hash": "6565f11bcbfbafe8f859e211d65b646e7cc642b41ee40910b2cf41f5ac47480b"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
