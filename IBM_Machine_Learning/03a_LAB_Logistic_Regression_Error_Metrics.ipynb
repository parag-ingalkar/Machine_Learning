{"cells":[{"cell_type":"markdown","id":"aac18046-cebf-436d-bedd-d081d74a1139","metadata":{},"outputs":[],"source":["# Machine Learning Foundation\n","\n","## Course 3, Part a: Logistic Regression LAB\n"]},{"cell_type":"markdown","id":"3632e430-1941-4f46-a10a-8858b578dc5d","metadata":{},"outputs":[],"source":["## Introduction\n","\n","We will be using the [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) database, which was built from the recordings of study participants who carried a smartphone with an embedded inertial sensor while performing activities of daily living (ADL). The objective is to classify the activities the participants performed into one of the six following categories: walking, walking upstairs, walking downstairs, sitting, standing, and laying.\n","\n","The following information is provided for each record in the dataset: \n","\n","- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration \n","- Triaxial Angular velocity from the gyroscope \n","- A 561-feature vector with time and frequency domain variables \n","- The activity label \n","\n","More information about the features are available on the website linked above.\n"]},{"cell_type":"markdown","id":"d266ce09-d2e6-418c-ba28-23a5963f63c3","metadata":{},"outputs":[],"source":["Install the below libraries\n"]},{"cell_type":"code","id":"6d3897be-3468-4b44-8b6b-0a2444efb185","metadata":{},"outputs":[],"source":["!pip install seaborn  \n!pip install pandas\n!pip install numpy\n!pip install scikit-learn"]},{"cell_type":"code","id":"751465de-8ce8-40ae-b8ae-b2cb1c32a2f1","metadata":{},"outputs":[],"source":["def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn"]},{"cell_type":"code","id":"ebe4223b-d710-41e0-8bf9-7e242386809f","metadata":{},"outputs":[],"source":["import seaborn as sns, pandas as pd, numpy as np"]},{"cell_type":"markdown","id":"7f123600-6e87-4440-9f90-cf34d0f0dd6c","metadata":{},"outputs":[],"source":["## Question 1\n","\n","Import the data and do the following:\n","\n","* Examine the data types--there are many columns, so it might be wise to use value counts.\n","* Determine if the floating point values need to be scaled.\n","* Determine the breakdown of each activity.\n","* Encode the activity label as an integer.\n"]},{"cell_type":"code","id":"d50433dd-e6cf-4ee3-a44c-2a5e09376138","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\ndata = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/Human_Activity_Recognition_Using_Smartphones_Data.csv\", sep=',')"]},{"cell_type":"markdown","id":"d2f32107-9a24-46d5-97a4-5ae9698e4301","metadata":{},"outputs":[],"source":["The data columns are all floats except for the activity label.\n"]},{"cell_type":"code","id":"d7bc97df-7d56-4b14-ad21-30a758f42979","metadata":{},"outputs":[],"source":["data.dtypes.value_counts()"]},{"cell_type":"code","id":"6207abe2-587a-4ead-879b-46db4a9b7066","metadata":{},"outputs":[],"source":["data.dtypes.tail()"]},{"cell_type":"markdown","id":"50b8bf93-ee2c-4d66-a406-fda1696ee3f7","metadata":{},"outputs":[],"source":["The data are all scaled from -1 (minimum) to 1.0 (maximum).\n"]},{"cell_type":"code","id":"1a40e426-6a3f-457d-ab84-d3873db518e2","metadata":{},"outputs":[],"source":["data.iloc[:, :-1].min().value_counts()"]},{"cell_type":"code","id":"6105a668-9afe-42b9-9d36-0c3189731c98","metadata":{},"outputs":[],"source":["data.iloc[:, :-1].max().value_counts()"]},{"cell_type":"markdown","id":"1287e7f8-27a8-41b5-9cc9-498a2bfe91a8","metadata":{},"outputs":[],"source":["Examine the breakdown of activities; they are relatively balanced.\n"]},{"cell_type":"code","id":"b212b79c-4f44-4979-8019-28eb64b2d62e","metadata":{},"outputs":[],"source":["data.Activity.value_counts()"]},{"cell_type":"markdown","id":"d195b9fa-53b4-46e2-a189-e5ec43689ca2","metadata":{},"outputs":[],"source":["Scikit learn classifiers won't accept a sparse matrix for the prediction column. Thus, either `LabelEncoder` needs to be used to convert the activity labels to integers, or if `DictVectorizer` is used, the resulting matrix must be converted to a non-sparse array.  \n","Use `LabelEncoder` to fit_transform the \"Activity\" column, and look at 5 random values.\n"]},{"cell_type":"code","id":"0f11d866-9827-434f-b472-24e769794054","metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndata['Activity'] = le.fit_transform(data.Activity)\ndata['Activity'].sample(5)\n### END SOLUTION"]},{"cell_type":"markdown","id":"1061ff28-cc8c-4909-abcc-faa00de32f85","metadata":{},"outputs":[],"source":["## Question 2\n","\n","* Calculate the correlations between the dependent variables.\n","* Create a histogram of the correlation values.\n","* Identify those that are most correlated (either positively or negatively).\n"]},{"cell_type":"code","id":"8d2ca058-c3fb-4c7a-a950-5304b7dcf231","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\n# Calculate the correlation values\nfeature_cols = data.columns[:-1]\ncorr_values = data[feature_cols].corr()\n\n# Simplify by emptying all the data below the diagonal\ntril_index = np.tril_indices_from(corr_values)\n\n# Make the unused values NaNs\nfor coord in zip(*tril_index):\n    corr_values.iloc[coord[0], coord[1]] = np.nan\n    \n# Stack the data and convert to a data frame\ncorr_values = (corr_values\n               .stack()\n               .to_frame()\n               .reset_index()\n               .rename(columns={'level_0':'feature1',\n                                'level_1':'feature2',\n                                0:'correlation'}))\n\n# Get the absolute values for sorting\ncorr_values['abs_correlation'] = corr_values.correlation.abs()"]},{"cell_type":"markdown","id":"64522b4d-2b71-4d02-8597-0ed2d4863d6f","metadata":{},"outputs":[],"source":["A histogram of the absolute value correlations.\n"]},{"cell_type":"code","id":"8df13a28-939e-4cf2-a131-e73fa993c146","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"]},{"cell_type":"code","id":"95a2e5ee-162a-4246-a26e-27ad83047814","metadata":{},"outputs":[],"source":["sns.set_context('talk')\nsns.set_style('white')\n\nax = corr_values.abs_correlation.hist(bins=50, figsize=(12, 8))\nax.set(xlabel='Absolute Correlation', ylabel='Frequency');"]},{"cell_type":"code","id":"e89d8984-8c73-4dae-9fcd-16292da81808","metadata":{},"outputs":[],"source":["# The most highly correlated values\ncorr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')\n### END SOLUTION"]},{"cell_type":"markdown","id":"ea27d163-ba6c-4bdf-b1f3-523f71cbf26c","metadata":{},"outputs":[],"source":["## Question 3\n","\n","* Split the data into train and test data sets. This can be done using any method, but consider using Scikit-learn's `StratifiedShuffleSplit` to maintain the same ratio of predictor classes.\n","* Regardless of the method used to split the data, compare the ratio of classes in both the train and test splits.\n"]},{"cell_type":"code","id":"a7399bd5-9088-4df0-9c14-791d30b096b6","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Get the split indexes\nstrat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n                                          test_size=0.3, \n                                          random_state=42)\n\ntrain_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.Activity))\n\n# Create the dataframes\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'Activity']\n\nX_test  = data.loc[test_idx, feature_cols]\ny_test  = data.loc[test_idx, 'Activity']"]},{"cell_type":"code","id":"cd01f68d-fda5-4d8f-bc94-5fc8549c4907","metadata":{},"outputs":[],"source":["y_train.value_counts(normalize=True)"]},{"cell_type":"code","id":"9ae85b07-a5ad-46bd-8349-f394d0b46f0d","metadata":{},"outputs":[],"source":["y_test.value_counts(normalize=True)\n### END SOLUTION"]},{"cell_type":"markdown","id":"050c50e7-56e3-446d-93e2-040b83fde09a","metadata":{},"outputs":[],"source":["## Question 4\n","\n","* Fit a logistic regression model without any regularization using all of the features. Be sure to read the documentation about fitting a multi-class model so you understand the coefficient output. Store the model.\n","* Using cross validation to determine the hyperparameters and fit models using L1 and L2 regularization. Store each of these models as well. Note the limitations on multi-class models, solvers, and regularizations. The regularized models, in particular the L1 model, will probably take a while to fit.\n"]},{"cell_type":"code","id":"69b0e439-a66e-43b4-afa6-84c1dd050d5c","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nfrom sklearn.linear_model import LogisticRegression\n\n# Standard logistic regression\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)"]},{"cell_type":"code","id":"8f8a7ee6-7994-41e2-8752-e65d6b250a72","metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegressionCV\n\n# L1 regularized logistic regression\nlr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)"]},{"cell_type":"code","id":"fefaf79a-9cba-4c90-9d20-aa97484ecc6c","metadata":{},"outputs":[],"source":["# L2 regularized logistic regression\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear').fit(X_train, y_train)\n### END SOLUTION"]},{"cell_type":"markdown","id":"2890639e-240c-4369-8663-05cd19782c54","metadata":{},"outputs":[],"source":["## Question 5\n","\n","* Compare the magnitudes of the coefficients for each of the models. If one-vs-rest fitting was used, each set of coefficients can be plotted separately. \n"]},{"cell_type":"code","id":"0ec43c9a-1861-416a-b6dd-cf9670f32e40","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\n# Combine all the coefficients into a dataframe\ncoefficients = list()\n\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\n\nfor lab,mod in zip(coeff_labels, coeff_models):\n    coeffs = mod.coef_\n    coeff_label = pd.MultiIndex(levels=[[lab], [0,1,2,3,4,5]], \n                                 codes=[[0,0,0,0,0,0], [0,1,2,3,4,5]])\n    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n\ncoefficients = pd.concat(coefficients, axis=1)\n\ncoefficients.sample(10)"]},{"cell_type":"markdown","id":"4684e913-dfe1-4c08-a81b-8814f740161d","metadata":{},"outputs":[],"source":["Prepare six separate plots for each of the multi-class coefficients.\n"]},{"cell_type":"code","id":"49b97063-d360-4f7b-ace2-eceacdc8ed34","metadata":{},"outputs":[],"source":["fig, axList = plt.subplots(nrows=3, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(10,10)\n\nfor ax in enumerate(axList):\n    loc = ax[0]\n    ax = ax[1]\n    \n    data = coefficients.xs(loc, level=1, axis=1)\n    data.plot(marker='o', ls='', ms=2.0, ax=ax, legend=False)\n    \n    if ax is axList[0]:\n        ax.legend(loc=4)\n        \n    ax.set(title='Coefficient Set '+str(loc))\n\nplt.tight_layout()\n### END SOLUTION"]},{"cell_type":"markdown","id":"690b0a68-e433-4f10-b90d-0d5936f10c65","metadata":{},"outputs":[],"source":["## Question 6\n","\n","* Predict and store the class for each model.\n","* Store the probability for the predicted class for each model. \n"]},{"cell_type":"code","id":"0160cfaf-b696-42ec-8195-89e3f6b14064","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\n# Predict the class and the probability for each\ny_pred = list()\ny_prob = list()\n\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\n\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n    \ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\n\ny_pred.head()"]},{"cell_type":"code","id":"e8a4a498-e4d5-4166-b3bc-0d4f78a8f094","metadata":{},"outputs":[],"source":["y_prob.head()\n### END SOLUTION"]},{"cell_type":"markdown","id":"112c6141-f9e4-4460-a39a-4fa2fe967f17","metadata":{},"outputs":[],"source":["## Question 7\n","\n","For each model, calculate the following error metrics: \n","\n","* Accuracy\n","* Precision\n","* Recall\n","* F-score\n","* Confusion Matrix\n","\n","Decide how to combine the multi-class metrics into a single value for each model.\n"]},{"cell_type":"code","id":"3f510ed2-e0b1-4bf4-a7f9-6cf433729c4e","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\nmetrics = list()\ncm = dict()\n\nfor lab in coeff_labels:\n\n    # Preciision, recall, f-score from the multi-class support function\n    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n    \n    # The usual way to calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred[lab])\n    \n    # ROC-AUC scores can be calculated by binarizing the data\n    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n              average='weighted')\n    \n    # Last, the confusion matrix\n    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n    \n    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, \n                             name=lab))\n\nmetrics = pd.concat(metrics, axis=1)"]},{"cell_type":"code","id":"9d803c69-afe1-4abb-a31d-47a185e9de4b","metadata":{},"outputs":[],"source":["metrics\n### END SOLUTION"]},{"cell_type":"markdown","id":"7541b8c2-2074-4489-bf28-25e3292aaefe","metadata":{},"outputs":[],"source":["## Question 8\n","\n","* Display or plot the confusion matrix for each model.\n"]},{"cell_type":"code","id":"afa8bace-7502-4e59-a76e-fb0b0111bf54","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nfig, axList = plt.subplots(nrows=2, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(12, 10)\n\naxList[-1].axis('off')\n\nfor ax,lab in zip(axList[:-1], coeff_labels):\n    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d');\n    ax.set(title=lab);\n    \nplt.tight_layout()\n### END SOLUTION"]},{"cell_type":"markdown","id":"917b78a1-b44d-4c1a-b2b9-f898180c36a4","metadata":{},"outputs":[],"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"6565f11bcbfbafe8f859e211d65b646e7cc642b41ee40910b2cf41f5ac47480b"},"nbformat":4,"nbformat_minor":4}